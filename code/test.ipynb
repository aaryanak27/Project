{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: emoji in /Users/aaryanshah/Library/Python/3.9/lib/python/site-packages (2.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "import glob \n",
    "import joblib\n",
    "from prettytable import PrettyTable\n",
    "from time import time\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Accessing, Cleaning, and Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_csv(file_path, chunk_size, output_dir):\n",
    "    # Extract the file name without extension to use in naming chunks\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "    with open(file_path, 'r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        headers = next(reader)\n",
    "\n",
    "        chunk_number = 1\n",
    "        chunk_file = None\n",
    "        chunk_writer = None\n",
    "        for i, row in enumerate(reader):\n",
    "            if i % chunk_size == 0:\n",
    "                if chunk_file is not None:\n",
    "                    chunk_file.close()\n",
    "                chunk_filename = f'{output_dir}/{base_name}_chunk_{chunk_number}.csv'\n",
    "                chunk_file = open(chunk_filename, 'w', newline='', encoding='utf-8')\n",
    "                chunk_writer = csv.writer(chunk_file)\n",
    "                chunk_writer.writerow(headers)\n",
    "                chunk_number += 1\n",
    "            chunk_writer.writerow(row)\n",
    "        if chunk_file is not None:\n",
    "            chunk_file.close()\n",
    "\n",
    "def reset():\n",
    "    df_games = pd.read_csv('../raw_data/raw_games.csv') \n",
    "    df_nh = pd.read_csv('../raw_data/raw_necessary_hardware.csv')\n",
    "    df_oc = pd.read_csv('../raw_data/raw_open_critic.csv')\n",
    "    df_sn = pd.read_csv('../raw_data/raw_social_networks.csv')\n",
    "    \n",
    "    df_games_chunk = pd.read_csv('../chunked_data/clean_games_chunk_1.csv')\n",
    "    df_nh_chunk = pd.read_csv('../chunked_data/clean_nh_chunk_1.csv')\n",
    "    df_oc_chunk = pd.read_csv('../chunked_data/clean_oc_chunk_1.csv')\n",
    "    df_sn_chunk = pd.read_csv('../chunked_data/clean_sn_chunk_1.csv')\n",
    "    \n",
    "    df_games = df_games.dropna()\n",
    "    df_nh = df_nh.dropna()\n",
    "    df_oc = df_oc.dropna()\n",
    "    df_sn = df_sn.dropna()\n",
    "    print('>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "    print(' ')\n",
    "    print('DATA RESETTED')\n",
    "    print(' ')\n",
    "    \n",
    "    print('>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "    \n",
    "\n",
    "    df_games.to_csv('../cleaned_data/clean_games.csv', index=False)\n",
    "    df_nh.to_csv('../cleaned_data/clean_nh.csv', index=False)\n",
    "    df_oc.to_csv('../cleaned_data/clean_oc.csv', index=False)\n",
    "    df_sn.to_csv('../cleaned_data/clean_sn.csv', index=False)\n",
    "    \n",
    "    chunk_csv('../cleaned_data/clean_games.csv', 500, '../chunked_data')     # Adjust chunk_size as needed\n",
    "    chunk_csv('../cleaned_data/clean_nh.csv', 500, '../chunked_data')     # Adjust chunk_size as needed\n",
    "    chunk_csv('../cleaned_data/clean_oc.csv', 500, '../chunked_data')     # Adjust chunk_size as needed\n",
    "    chunk_csv('../cleaned_data/clean_sn.csv', 500, '../chunked_data')     # Adjust chunk_size as needed\n",
    "\n",
    "def memory_usage():\n",
    "    df_games = pd.read_csv('../raw_data/raw_games.csv') \n",
    "    df_nh = pd.read_csv('../raw_data/raw_necessary_hardware.csv')\n",
    "    df_oc = pd.read_csv('../raw_data/raw_open_critic.csv')\n",
    "    df_sn = pd.read_csv('../raw_data/raw_social_networks.csv')\n",
    "    \n",
    "    df_games_chunk = pd.read_csv('../chunked_data/clean_games_chunk_1.csv')\n",
    "    df_nh_chunk = pd.read_csv('../chunked_data/clean_nh_chunk_1.csv')\n",
    "    df_oc_chunk = pd.read_csv('../chunked_data/clean_oc_chunk_1.csv')\n",
    "    df_sn_chunk = pd.read_csv('../chunked_data/clean_sn_chunk_1.csv')\n",
    "    \n",
    "    print(df_games.info(memory_usage='deep'))\n",
    "    \n",
    "    print('>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "    \n",
    "    print(df_nh.info(memory_usage='deep'))\n",
    "    \n",
    "    print('>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "    \n",
    "    print(df_oc.info(memory_usage='deep'))\n",
    "    \n",
    "    print('>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "    \n",
    "    print(df_sn.info(memory_usage='deep'))\n",
    "    \n",
    "    print('>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "    print('>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "    \n",
    "    print(df_games_chunk.info(memory_usage='deep'))\n",
    "    print('>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "    \n",
    "    print(df_nh_chunk.info(memory_usage='deep'))\n",
    "    print('>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "    \n",
    "    print(df_oc_chunk.info(memory_usage='deep'))\n",
    "    print('>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "    \n",
    "    print(df_sn_chunk.info(memory_usage='deep'))\n",
    "    print('>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 915 entries, 0 to 914\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   id            915 non-null    object\n",
      " 1   name          915 non-null    object\n",
      " 2   game_slug     915 non-null    object\n",
      " 3   price         915 non-null    int64 \n",
      " 4   release_date  915 non-null    object\n",
      " 5   platform      783 non-null    object\n",
      " 6   description   915 non-null    object\n",
      " 7   developer     712 non-null    object\n",
      " 8   publisher     707 non-null    object\n",
      " 9   genres        757 non-null    object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 811.2 KB\n",
      "None\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1765 entries, 4c81547b81064acfb1902be7b06d63661 to c7372a04d62b4d4bb5b2a95424202e252\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   id                  1615 non-null   object\n",
      " 1   operacional_system  1634 non-null   object\n",
      " 2   processor           1618 non-null   object\n",
      " 3   memory              1397 non-null   object\n",
      " 4   graphics            1341 non-null   object\n",
      " 5   fk_game_id          1765 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 961.2 KB\n",
      "None\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17584 entries, 0 to 17583\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   id          17584 non-null  object \n",
      " 1   company     17584 non-null  object \n",
      " 2   author      15769 non-null  object \n",
      " 3   rating      17053 non-null  float64\n",
      " 4   comment     17428 non-null  object \n",
      " 5   date        17584 non-null  object \n",
      " 6   top_critic  17584 non-null  bool   \n",
      " 7   game_id     17584 non-null  object \n",
      "dtypes: bool(1), float64(1), object(6)\n",
      "memory usage: 14.2 MB\n",
      "None\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3045 entries, 0 to 3044\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   id           3045 non-null   int64 \n",
      " 1   description  3045 non-null   object\n",
      " 2   url          3045 non-null   object\n",
      " 3   fk_game_id   3045 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 768.7 KB\n",
      "None\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   id            500 non-null    object\n",
      " 1   name          500 non-null    object\n",
      " 2   game_slug     500 non-null    object\n",
      " 3   price         500 non-null    int64 \n",
      " 4   release_date  500 non-null    object\n",
      " 5   platform      500 non-null    object\n",
      " 6   description   500 non-null    object\n",
      " 7   developer     500 non-null    object\n",
      " 8   publisher     500 non-null    object\n",
      " 9   genres        500 non-null    object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 458.8 KB\n",
      "None\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   id                  500 non-null    object\n",
      " 1   operacional_system  500 non-null    object\n",
      " 2   processor           500 non-null    object\n",
      " 3   memory              500 non-null    object\n",
      " 4   graphics            500 non-null    object\n",
      " 5   fk_game_id          500 non-null    object\n",
      "dtypes: object(6)\n",
      "memory usage: 251.8 KB\n",
      "None\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   id          500 non-null    object \n",
      " 1   company     500 non-null    object \n",
      " 2   author      500 non-null    object \n",
      " 3   rating      500 non-null    float64\n",
      " 4   comment     500 non-null    object \n",
      " 5   date        500 non-null    object \n",
      " 6   top_critic  500 non-null    bool   \n",
      " 7   game_id     500 non-null    object \n",
      "dtypes: bool(1), float64(1), object(6)\n",
      "memory usage: 408.2 KB\n",
      "None\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   id           500 non-null    int64 \n",
      " 1   description  500 non-null    object\n",
      " 2   url          500 non-null    object\n",
      " 3   fk_game_id   500 non-null    object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 126.7 KB\n",
      "None\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    }
   ],
   "source": [
    "memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import csv\n",
    "import glob\n",
    "import joblib\n",
    "from time import time\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# Parse SQL Query   \n",
    "def parse_sql_query(sql_query):\n",
    "    # Initialize condition and from_part variables\n",
    "    condition = None\n",
    "    from_part = None\n",
    "    select_part = None\n",
    "\n",
    "    # Split the query into SELECT and FROM parts, and optionally WHERE\n",
    "    if \"SELECT\" in sql_query:\n",
    "        # Split SELECT and FROM parts\n",
    "        select_part, rest_of_query = sql_query.split(\" FROM \")\n",
    "        columns = select_part.replace(\"SELECT \", \"\").strip()\n",
    "\n",
    "        # Check if all columns are requested with a wildcard '*'\n",
    "        if columns == '*' or columns == 'all':\n",
    "            columns = None\n",
    "        else:\n",
    "            columns = columns.split(\", \")\n",
    "\n",
    "        # Initialize variables for WHERE and ORDER BY clauses\n",
    "        where_condition = None\n",
    "        order_by_column = None\n",
    "        is_ascending = True  # Default sort order\n",
    "\n",
    "        # Split WHERE (if exists) and ORDER BY parts\n",
    "        if \" WHERE \" in rest_of_query:\n",
    "            from_part, where_part = rest_of_query.split(\" WHERE \")\n",
    "            table_name = from_part.strip()\n",
    "\n",
    "            if \" ORDER BY \" in where_part:\n",
    "                where_condition, order_by_part = where_part.split(\" ORDER BY \")\n",
    "                order_by_column = order_by_part.strip()\n",
    "            else:\n",
    "                where_condition = where_part.strip()\n",
    "        elif \" ORDER BY \" in rest_of_query:\n",
    "            table_name, order_by_part = rest_of_query.split(\" ORDER BY \")\n",
    "            order_by_column = order_by_part.strip()\n",
    "        else:\n",
    "            table_name = rest_of_query.strip()\n",
    "\n",
    "        # Check for ascending or descending order\n",
    "        if order_by_column:\n",
    "            if \" DESC\" in order_by_column:\n",
    "                is_ascending = False\n",
    "                order_by_column = order_by_column.replace(\" DESC\", \"\").strip()\n",
    "            else:\n",
    "                order_by_column = order_by_column.replace(\" ASC\", \"\").strip()\n",
    "\n",
    "        # Execute the query based on the parsed components\n",
    "        select_from_table(table_name, columns, where_condition, order_by_column, is_ascending)\n",
    "        # print_sorted_data(result)\n",
    "            \n",
    "    if \"INSERT INTO\" in sql_query:\n",
    "        print(\"inserting\")\n",
    "        insert_part, values_part = sql_query.split(\" VALUES \")\n",
    "        table_name = insert_part.replace(\"INSERT INTO \", \"\").strip()\n",
    "        values = values_part.strip(\"()\").replace(\"'\",\"\").split(\",\")\n",
    "        #calling function to insert into table\n",
    "        insert_into_table(table_name, values)\n",
    "        \n",
    "    if \"DELETE\" in sql_query:\n",
    "        delete_part, where_part = sql_query.split(\"WHERE\")\n",
    "        table_name = delete_part.replace(\"DELETE FROM\",'').strip()\n",
    "        condition = where_part.strip()\n",
    "        # calling function to delete from table        \n",
    "        delete_from_table(table_name, condition)\n",
    "    \n",
    "    if \"UPDATE\" in sql_query: \n",
    "        x,y = sql_query.split(\"SET\")\n",
    "        table_name = x.replace(\"UPDATE\", \"\").strip()\n",
    "        setp, condition = y.split(\"WHERE\")\n",
    "        condition = condition.strip()\n",
    "        k,v = setp.replace(\" \", \"\").split(\"=\")\n",
    "        # typecasting setter value to correct type\n",
    "        if v.isnumeric():\n",
    "            v = int(v)\n",
    "        elif v.replace('.', '', 1).isdigit():\n",
    "            v = float(v)\n",
    "        else:\n",
    "            pass    \n",
    "        setter = (k,v)\n",
    "        # calling function to update table\n",
    "        update_table(table_name, setter, condition)\n",
    "\n",
    "    if \"AVG\" in sql_query:\n",
    "        avg_part, from_part = sql_query.split(\"FROM\")\n",
    "        column_name = avg_part.replace(\"AVG\", \"\").strip()\n",
    "        table_name = from_part.strip()\n",
    "        # calling function to calculate average\n",
    "        average(table_name, column_name)\n",
    "        \n",
    "    if \"SUM\" in sql_query:\n",
    "        sum_part, from_part = sql_query.split(\"FROM\")\n",
    "        column_name = sum_part.replace(\"SUM\", \"\").strip()\n",
    "        table_name = from_part.strip()\n",
    "        # calling function to calculate sum\n",
    "        sum(table_name, column_name)\n",
    "        \n",
    "    if \"MIN\" in sql_query:\n",
    "        min_part, from_part = sql_query.split(\"FROM\")\n",
    "        column_name = min_part.replace(\"MIN\", \"\").strip()\n",
    "        table_name = from_part.strip()\n",
    "        # calling function to calculate minimum\n",
    "        min(table_name, column_name)\n",
    "    \n",
    "    if \"MAX\" in sql_query:\n",
    "        max_part, from_part = sql_query.split(\"FROM\")\n",
    "        column_name = max_part.replace(\"MAX\", \"\").strip()\n",
    "        table_name = from_part.strip()\n",
    "        # calling function to calculate maximum\n",
    "        max(table_name, column_name)\n",
    "        \n",
    "    if \"COUNT\" in sql_query:\n",
    "        count_part, from_part = sql_query.split(\" FROM \")\n",
    "        table_name = from_part.strip()\n",
    "        # Check if a WHERE clause is present\n",
    "        if from_part and \" WHERE \" in from_part:\n",
    "            from_part, where_part = from_part.split(\" WHERE \")\n",
    "            table_name = from_part.strip()\n",
    "            condition = where_part.strip()\n",
    "            \n",
    "        select_columns = count_part.replace(\"COUNT \", \"\").strip()\n",
    "        # Check if all columns are requested with a wildcard '*' or 'all'\n",
    "        if select_columns == '*' or select_columns == 'all':\n",
    "            columns = None\n",
    "        else:\n",
    "            columns = select_columns\n",
    "        # calling function to select from table\n",
    "        count(table_name, columns, condition)\n",
    "\n",
    "    if \"JOIN\" in sql_query:\n",
    "        parts = sql_query.split()\n",
    "\n",
    "        # Extract table names and join column\n",
    "        table1_name = parts[0]\n",
    "        table2_name = parts[2]\n",
    "        join_condition = parts[4]  # \"table1.column = table2.column\"\n",
    "\n",
    "        join_column = join_condition.split('=')[0].split('.')[1]  # Assuming format \"table.column\"\n",
    "\n",
    "        # Initialize optional parts\n",
    "        where_condition = None\n",
    "        order_by_column = None\n",
    "        is_ascending = True  # Default sort order\n",
    "\n",
    "        # Check for WHERE clause\n",
    "        if \"WHERE\" in sql_query:\n",
    "            where_index = parts.index(\"WHERE\")\n",
    "            where_condition = \" \".join(parts[where_index + 1:])\n",
    "\n",
    "        # Check for ORDER BY clause\n",
    "        if \"ORDER BY\" in sql_query:\n",
    "            order_by_index = parts.index(\"ORDER\") + 2\n",
    "            order_by_column = parts[order_by_index]\n",
    "            if len(parts) > order_by_index + 1 and parts[order_by_index + 1].upper() == \"DESC\":\n",
    "                is_ascending = False\n",
    "        # calling the sql join function\n",
    "        sql_join(table1_name, table2_name, join_column, where_condition, order_by_column, is_ascending)\n",
    "        \n",
    "# selecting from table function which takes table name, columns and condition as input\n",
    "def select_from_table(table_name, columns, condition, order_by_column=None, is_ascending=True):\n",
    "    # Find all chunk files for the table\n",
    "    chunk_files_pattern = f'../chunked_data/clean_{table_name}_chunk_*.csv'\n",
    "    chunk_files = glob.glob(chunk_files_pattern)\n",
    "    if not chunk_files:\n",
    "        raise FileNotFoundError(f\"No files found for the table {table_name}.\")\n",
    "\n",
    "    # Start with an empty list to store selected rows\n",
    "    selected_rows = []\n",
    "\n",
    "    # Create a metadata dictionary to store the data types of each column\n",
    "    metadata = {'type': {}}\n",
    "\n",
    "    with open(chunk_files[0], 'r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        if columns is None:\n",
    "            columns = reader.fieldnames\n",
    "\n",
    "        _r = next(reader)\n",
    "        for k, v in _r.items():\n",
    "            if v.isnumeric():\n",
    "                metadata['type'][k] = int\n",
    "            elif v.replace('.', '', 1).isdigit():\n",
    "                metadata['type'][k] = float\n",
    "            else:\n",
    "                metadata['type'][k] = str\n",
    "\n",
    "    st = time()  # Start time\n",
    "\n",
    "    # Function to process each chunk file\n",
    "    def process_chunk(file_path):\n",
    "        rows = []\n",
    "        with open(file_path, 'r', newline='', encoding='utf-8') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            for row in reader:\n",
    "                _lcl = {}\n",
    "                for k, v in row.items():\n",
    "                    if metadata['type'][k] == int:\n",
    "                        _lcl[k] = int(v)\n",
    "                    elif metadata['type'][k] == float:\n",
    "                        _lcl[k] = float(v)\n",
    "                    else:\n",
    "                        _lcl[k] = v\n",
    "\n",
    "                if condition is None or eval(condition, {}, _lcl):\n",
    "                    rows.append({col: row[col] for col in columns})\n",
    "        return rows\n",
    "\n",
    "    # Process each chunk file in parallel using joblib\n",
    "    results = joblib.Parallel(n_jobs=-1)(joblib.delayed(process_chunk)(file_path) for file_path in chunk_files)\n",
    "\n",
    "    # Flatten the results and store in selected_rows\n",
    "    for rows in results:\n",
    "        selected_rows.extend(rows)\n",
    "\n",
    "    # Sort the data based on the order_by_column\n",
    "    if order_by_column:\n",
    "        selected_rows.sort(key=lambda x: metadata['type'][order_by_column](x[order_by_column]), reverse=not is_ascending)\n",
    "\n",
    "    et = time()  # End time\n",
    "    print(f\"Time taken to process {len(chunk_files)} chunk files: {et - st:.2f} seconds\")\n",
    "\n",
    "    # Print the table using PrettyTable\n",
    "    table = PrettyTable()\n",
    "    table.field_names = columns\n",
    "    for row in selected_rows:\n",
    "        table.add_row([row[col] for col in columns])\n",
    "\n",
    "    print(table)\n",
    "    # result_text_box.value = str(table)\n",
    "    \n",
    "    ett = time()  # End time\n",
    "    print(f\"Time taken to print {len(chunk_files)} chunk files: {ett - et:.2f} seconds\")\n",
    "\n",
    "# insert into table function which takes table name and values as input\n",
    "def insert_into_table(table_name, values):\n",
    "    # Find the latest chunk file for the table\n",
    "    chunk_files_pattern = f'../chunked_data/clean_{table_name}_chunk_*.csv'\n",
    "    chunk_files = glob.glob(chunk_files_pattern)\n",
    "    if not chunk_files:\n",
    "        raise FileNotFoundError(f\"No files found for the table {table_name}.\")\n",
    "    latest_chunk_file = max(chunk_files, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "\n",
    "    print(f\"inserting {values}\")\n",
    "    # Append the new values to the latest chunk file\n",
    "    with open(latest_chunk_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(values)\n",
    "\n",
    "# delete from table function which takes table name and condition as input\n",
    "def delete_from_table(table_name, condition):\n",
    "    # Find all chunk files for the table\n",
    "    chunk_files_pattern = f'../chunked_data/clean_{table_name}_chunk_*.csv'\n",
    "    chunk_files = glob.glob(chunk_files_pattern)\n",
    "    if not chunk_files:\n",
    "        raise FileNotFoundError(f\"No files found for the table {table_name}.\")\n",
    "    # Rest of the code...\n",
    "    chunk_files_pattern = f'../chunked_data/clean_{table_name}_chunk_*.csv'\n",
    "    chunk_files = glob.glob(chunk_files_pattern)\n",
    "    if not chunk_files:\n",
    "        raise FileNotFoundError(f\"No files found for the table {table_name}.\")\n",
    "    \n",
    "    metadata = {}\n",
    "    metadata['type'] = {}\n",
    "    columns = []\n",
    "    with open(chunk_files[0], 'r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        columns = reader.fieldnames\n",
    "\n",
    "        _r = next(reader)\n",
    "        for k, v in _r.items():\n",
    "            if v.isnumeric():\n",
    "                metadata['type'][k] = int\n",
    "            elif v.replace('.', '', 1).isdigit():\n",
    "                metadata['type'][k] = float\n",
    "            else:\n",
    "                metadata['type'][k] = str\n",
    "    \n",
    "    # Delete rows from each chunk file that satisfy the condition\n",
    "    for file_path in chunk_files:\n",
    "        rows_to_keep = []\n",
    "        rows_to_delete = []\n",
    "        with open(file_path, 'r', newline='', encoding='utf-8') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            total_rows = 0\n",
    "            for row in reader:\n",
    "                total_rows += 1\n",
    "                _lcl = {}\n",
    "                for k, v in row.items():\n",
    "                    if metadata['type'][k] == int:\n",
    "                        _lcl[k] = int(v)\n",
    "                    elif metadata['type'][k] == float:\n",
    "                        _lcl[k] = float(v)\n",
    "                    else:\n",
    "                        _lcl[k] = v\n",
    "                if not eval(condition, {}, _lcl):\n",
    "                    rows_to_keep.append(row)\n",
    "                else:\n",
    "                    rows_to_delete.append(row)\n",
    "                    \n",
    "            chunk_name = file_path.split('_')[-1].split('.')[0]  \n",
    "            print(f\"rows to delete : {total_rows - len(rows_to_keep)} in chunk #{chunk_name}\")\n",
    "           \n",
    "        with open(file_path, 'w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=columns)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rows_to_keep)\n",
    "\n",
    "# update table function which takes table name, setter and condition as input\n",
    "def update_table(table_name, setter, condition):\n",
    "    # Find all chunk files for the table\n",
    "    chunk_files_pattern = f'../chunked_data/clean_{table_name}_chunk_*.csv'\n",
    "    chunk_files = glob.glob(chunk_files_pattern)\n",
    "    if not chunk_files:\n",
    "        raise FileNotFoundError(f\"No files found for the table {table_name}.\")\n",
    "    # Rest of the code...\n",
    "    chunk_files_pattern = f'../chunked_data/clean_{table_name}_chunk_*.csv'\n",
    "    chunk_files = glob.glob(chunk_files_pattern)\n",
    "    if not chunk_files:\n",
    "        raise FileNotFoundError(f\"No files found for the table {table_name}.\")\n",
    "    \n",
    "    metadata = {}\n",
    "    metadata['type'] = {}\n",
    "    columns = []\n",
    "    with open(chunk_files[0], 'r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        columns = reader.fieldnames\n",
    "\n",
    "        _r = next(reader)\n",
    "        for k, v in _r.items():\n",
    "            if v.isnumeric():\n",
    "                metadata['type'][k] = int\n",
    "            elif v.replace('.', '', 1).isdigit():\n",
    "                metadata['type'][k] = float\n",
    "            else:\n",
    "                metadata['type'][k] = str\n",
    "    \n",
    "    # Delete rows from each chunk file that satisfy the condition\n",
    "    for file_path in chunk_files:\n",
    "        updated_rows = []\n",
    "        r_upd = 0\n",
    "        with open(file_path, 'r', newline='', encoding='utf-8') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            total_rows = 0\n",
    "            for row in reader:\n",
    "                total_rows += 1\n",
    "                _lcl = {}\n",
    "                for k, v in row.items():\n",
    "                    if metadata['type'][k] == int:\n",
    "                        _lcl[k] = int(v)\n",
    "                    elif metadata['type'][k] == float:\n",
    "                        _lcl[k] = float(v)\n",
    "                    else:\n",
    "                        _lcl[k] = v\n",
    "                if eval(condition, {}, _lcl):\n",
    "                    # update logic\n",
    "                    r_upd += 1\n",
    "                    row[setter[0]] = setter[1]\n",
    "                    pass  \n",
    "                updated_rows.append(row)\n",
    "   \n",
    "        print(f\"rows updated {r_upd}\")        \n",
    "        with open(file_path, 'w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=columns)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(updated_rows)\n",
    "            \n",
    "# Average function calculates average of a column in a table and takes input table name and column name\n",
    "def average(table_name, column_name):\n",
    "    chunk_files_pattern = f'../chunked_data/clean_{table_name}_chunk_*.csv'\n",
    "    chunk_files = glob.glob(chunk_files_pattern)\n",
    "    if not chunk_files:\n",
    "        raise FileNotFoundError(f\"No files found for the table {table_name}.\")\n",
    "    \n",
    "    total_sum = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    # Calculate the sum and count of the selected column from each chunk file\n",
    "    for file_path in chunk_files:\n",
    "        with open(file_path, 'r', newline='', encoding='utf-8') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            for row in reader:\n",
    "                if column_name in row:\n",
    "                    value = row[column_name]\n",
    "                    if value.isdigit():\n",
    "                        total_sum += int(value)\n",
    "                        total_count += 1\n",
    "                    elif value.replace('.', '', 1).isdigit():\n",
    "                        total_sum += float(value)\n",
    "                        total_count += 1\n",
    "    \n",
    "    # Calculate the average\n",
    "    if total_count > 0:\n",
    "        average = total_sum / total_count\n",
    "        print(f\"Average of {column_name} is {average}\") \n",
    "    else:\n",
    "        print(f\"No values found for {column_name}\")\n",
    "    \n",
    "# Sum function calculates sum of a column in a table and takes input table name and column name\n",
    "def sum(table_name, column_name):\n",
    "    chunk_files_pattern = f'../chunked_data/clean_{table_name}_chunk_*.csv'\n",
    "    chunk_files = glob.glob(chunk_files_pattern)\n",
    "    if not chunk_files:\n",
    "        raise FileNotFoundError(f\"No files found for the table {table_name}.\")\n",
    "    \n",
    "    total_sum = 0\n",
    "    \n",
    "    # Calculate the sum and count of the selected column from each chunk file\n",
    "    for file_path in chunk_files:\n",
    "        with open(file_path, 'r', newline='', encoding='utf-8') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            for row in reader:\n",
    "                if column_name in row:\n",
    "                    value = row[column_name]\n",
    "                    if value.isdigit():\n",
    "                        total_sum += int(value)\n",
    "                    elif value.replace('.', '', 1).isdigit():\n",
    "                        total_sum += float(value)\n",
    "    \n",
    "    # Calculate the average\n",
    "    if total_sum > 0:\n",
    "        print(f\"Sum of {column_name} is {total_sum}\") \n",
    "    else:\n",
    "        print(f\"No values found for {column_name}\")\n",
    "    \n",
    "# Min function calculates minimum of a column in a table and takes input table name and column name\n",
    "def min(table_name, column_name):\n",
    "    chunk_files_pattern = f'../chunked_data/clean_{table_name}_chunk_*.csv'\n",
    "    chunk_files = glob.glob(chunk_files_pattern)\n",
    "    if not chunk_files:\n",
    "        raise FileNotFoundError(f\"No files found for the table {table_name}.\")\n",
    "    \n",
    "    min_value = None\n",
    "    \n",
    "    # min of the selected column from each chunk file\n",
    "    for file_path in chunk_files:\n",
    "        with open(file_path, 'r', newline='', encoding='utf-8') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            for row in reader:\n",
    "                if column_name in row:\n",
    "                    value = row[column_name]\n",
    "                    if value.isdigit():\n",
    "                        value = int(value)\n",
    "                    elif value.replace('.', '', 1).isdigit():\n",
    "                        value = float(value)\n",
    "                    if min_value is None or value < min_value:\n",
    "                        min_value = value\n",
    "    \n",
    "    # Calculate the average\n",
    "    if min_value is not None:\n",
    "        print(f\"Minimum of {column_name} is {min_value}\") \n",
    "    else:\n",
    "        print(f\"No values found for {column_name}\")\n",
    "        \n",
    "# Max function calculates maximum of a column in a table and takes input table name and column name\n",
    "def max(table_name, column_name):\n",
    "    chunk_files_pattern = f'../chunked_data/clean_{table_name}_chunk_*.csv'\n",
    "    chunk_files = glob.glob(chunk_files_pattern)\n",
    "    if not chunk_files:\n",
    "        raise FileNotFoundError(f\"No files found for the table {table_name}.\")\n",
    "    max_value = None\n",
    "    # max of the selected column from each chunk file\n",
    "    for file_path in chunk_files:\n",
    "        with open(file_path, 'r', newline='', encoding='utf-8') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            for row in reader:\n",
    "                if column_name in row:\n",
    "                    value = row[column_name]\n",
    "                    if value.isdigit():\n",
    "                        value = int(value)\n",
    "                    elif value.replace('.', '', 1).isdigit():\n",
    "                        value = float(value)\n",
    "                    if max_value is None or value > max_value:\n",
    "                        max_value = value\n",
    "    if max_value is not None:\n",
    "        print(f\"Maximum of {column_name} is {max_value}\") \n",
    "    else:\n",
    "        print(f\"No values found for {column_name}\")\n",
    "        \n",
    "# Count function calculates count of a value in column in a table and takes input table name, column name and condition\n",
    "def count(table_name, column_name, condition):\n",
    "    # Find all chunk files for the table\n",
    "    chunk_files_pattern = f'../chunked_data/clean_{table_name}_chunk_*.csv'\n",
    "    chunk_files = glob.glob(chunk_files_pattern)\n",
    "    if not chunk_files:\n",
    "        raise FileNotFoundError(f\"No files found for the table {table_name}.\")\n",
    "\n",
    "    metadata = {}\n",
    "    metadata['type'] = {}\n",
    "    columns = []\n",
    "    with open(chunk_files[0], 'r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        columns = reader.fieldnames\n",
    "\n",
    "        _r = next(reader)\n",
    "        for k, v in _r.items():\n",
    "            if v.isnumeric():\n",
    "                metadata['type'][k] = int\n",
    "            elif v.replace('.', '', 1).isdigit():\n",
    "                metadata['type'][k] = float\n",
    "            else:\n",
    "                metadata['type'][k] = str\n",
    "    \n",
    "    # Delete rows from each chunk file that satisfy the condition\n",
    "    for file_path in chunk_files:\n",
    "        count = 0\n",
    "        with open(file_path, 'r', newline='', encoding='utf-8') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            total_rows = 0\n",
    "            for row in reader:\n",
    "                total_rows += 1\n",
    "                _lcl = {}\n",
    "                for k, v in row.items():\n",
    "                    if metadata['type'][k] == int:\n",
    "                        _lcl[k] = int(v)\n",
    "                    elif metadata['type'][k] == float:\n",
    "                        _lcl[k] = float(v)\n",
    "                    else:\n",
    "                        _lcl[k] = v\n",
    "                if eval(condition, {}, _lcl):\n",
    "                    count += 1\n",
    "        chunk_name = file_path.split('_')[-1].split('.')[0] \n",
    "        print(f\"count : {count} in chunk #{chunk_name}\")\n",
    "# Join function joins two tables and takes input table names, join column, where condition, order by column and sort order\n",
    "def sql_join(table1_name, table2_name, join_column, where_condition=None, order_by_column=None, is_ascending=True):\n",
    "    def read_chunked_data(chunk_files_pattern):\n",
    "        chunk_files = glob.glob(chunk_files_pattern)\n",
    "        if not chunk_files:\n",
    "            raise FileNotFoundError(f\"No files found for the pattern {chunk_files_pattern}\")\n",
    "\n",
    "        data = []\n",
    "        for file_path in chunk_files:\n",
    "            with open(file_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "                reader = csv.DictReader(csvfile)\n",
    "                data.extend(list(reader))\n",
    "        return data\n",
    "    # Read data from chunked files\n",
    "    table1_data = read_chunked_data(f'../chunked_data/clean_{table1_name}_chunk_*.csv')\n",
    "    table2_data = read_chunked_data(f'../chunked_data/clean_{table2_name}_chunk_*.csv')\n",
    "\n",
    "    # Create a lookup dictionary for one of the tables\n",
    "    table2_lookup = {}\n",
    "    for row in table2_data:\n",
    "        key = row[join_column]\n",
    "        if key in table2_lookup:\n",
    "            table2_lookup[key].append(row)\n",
    "        else:\n",
    "            table2_lookup[key] = [row]\n",
    "\n",
    "    # Perform the join operation\n",
    "    joined_data = []\n",
    "    for row1 in table1_data:\n",
    "        key = row1[join_column]\n",
    "        if key in table2_lookup:\n",
    "            for row2 in table2_lookup[key]:\n",
    "                combined_row = {**row1, **row2}\n",
    "                if where_condition is None or eval(where_condition, {}, combined_row):\n",
    "                    joined_data.append(combined_row)\n",
    "\n",
    "    # Sort the data if ORDER BY clause is specified\n",
    "    if order_by_column:\n",
    "        joined_data = sorted(joined_data, key=lambda x: float(x[order_by_column]) if x[order_by_column].replace('.', '', 1).isdigit() else x[order_by_column], reverse=not is_ascending)\n",
    "\n",
    "    if joined_data:\n",
    "        print(tabulate(joined_data, headers=\"keys\"))\n",
    "    else:\n",
    "        print(\"No data to print.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      " \n",
      "DATA RESETTED\n",
      " \n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    }
   ],
   "source": [
    "reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: emoji in /Users/aaryanshah/Library/Python/3.9/lib/python/site-packages (2.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 21:18:21.224 Python[10800:1039257] WARNING: Secure coding is automatically enabled for restorable state! However, not on all supported macOS versions of this application. Opt-in to secure coding explicitly by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState:.\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/tkinter/__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/var/folders/5k/16d0ycbn02xdmkn6gb6nkfr00000gn/T/ipykernel_10800/3683339881.py\", line 58, in parse_query\n",
      "    parse_sql_query(sql_query)\n",
      "NameError: name 'parse_sql_query' is not defined\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/tkinter/__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/var/folders/5k/16d0ycbn02xdmkn6gb6nkfr00000gn/T/ipykernel_10800/3683339881.py\", line 58, in parse_query\n",
      "    parse_sql_query(sql_query)\n",
      "NameError: name 'parse_sql_query' is not defined\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/tkinter/__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/var/folders/5k/16d0ycbn02xdmkn6gb6nkfr00000gn/T/ipykernel_10800/3683339881.py\", line 58, in parse_query\n",
      "    parse_sql_query(sql_query)\n",
      "NameError: name 'parse_sql_query' is not defined\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/tkinter/__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/var/folders/5k/16d0ycbn02xdmkn6gb6nkfr00000gn/T/ipykernel_10800/1074743887.py\", line 96, in <lambda>\n",
      "    button = tk.Button(root, text=f\"{shortcode} {emoji_char}\", command=lambda sc=shortcode: add_emoji_to_query(sc))\n",
      "  File \"/var/folders/5k/16d0ycbn02xdmkn6gb6nkfr00000gn/T/ipykernel_10800/3683339881.py\", line 48, in add_emoji_to_query\n",
      "    query_text_box.insert(tk.END, emoji_shortcut)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/tkinter/__init__.py\", line 3743, in insert\n",
      "    self.tk.call((self._w, 'insert', index, chars) + args)\n",
      "_tkinter.TclError: invalid command name \".!frame.!scrolledtext\"\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "import emoji\n",
    "\n",
    "emoji_sql_mapping = {\n",
    "    emoji.emojize(':magnifying_glass_tilted_right:'): 'SELECT',                 # 🔍 works\n",
    "    emoji.emojize(':file_folder:'): 'FROM',                                     # 📁 works\n",
    "    emoji.emojize(':red_question_mark:'): 'WHERE',                              # ❓ works\n",
    "    emoji.emojize(':bar_chart:'): 'ORDER BY',                                   # 📊 works\n",
    "    emoji.emojize(':cross_mark:'): 'DELETE',                                    # ❌ works\n",
    "    emoji.emojize(':envelope_with_arrow:'): 'INSERT INTO',                      # 📩 works\n",
    "    emoji.emojize(':pencil:'): 'VALUES',                                        # ✏ works \n",
    "    emoji.emojize(':newspaper:'): 'UPDATE',                                     # 📰 works\n",
    "    emoji.emojize(':check_box_with_check:'): 'SET',                             # ✅ works\n",
    "    emoji.emojize(':plus:'): 'SUM',                                             # ➕ works\n",
    "    emoji.emojize(':money_with_wings:'): 'AVG',                                 # 💸 works\n",
    "    emoji.emojize(':input_numbers:'): 'COUNT',                                  # 🔢 works\n",
    "    emoji.emojize(':down_arrow:'): 'MIN',                                       # ⬇ works\n",
    "    emoji.emojize(':up_arrow:'): 'MAX',                                         # ⬆ works\n",
    "    emoji.emojize(':handshake:'): 'JOIN',                                       # 🤝 works\n",
    "    emoji.emojize(':speaker_low_volume::speaker_high_volume:'): 'ASC',          # 🔉🔊 works\n",
    "    emoji.emojize(':speaker_high_volume::speaker_low_volume:'): 'DESC',         # 🔊🔉 works\n",
    "    # Add more mappings as required\n",
    "}\n",
    "\n",
    "delete_qury = \"🗑 FROM games WHERE price == 999\"\n",
    "\n",
    "\n",
    "# Parse Emoji Input\n",
    "def parse_emoji_query(emoji_query):\n",
    "    # Split the query into tokens\n",
    "    tokens = emoji_query.split()\n",
    "\n",
    "    # Translate each token\n",
    "    translated_tokens = []\n",
    "    for token in tokens:\n",
    "        if token in emoji_sql_mapping:\n",
    "            translated_tokens.append(emoji_sql_mapping[token])\n",
    "        else:\n",
    "            translated_tokens.append(token)\n",
    "\n",
    "    # Reconstruct the query\n",
    "    sql_query = \" \".join(translated_tokens)\n",
    "    return sql_query\n",
    "\n",
    "# Function to add emoji to the text box\n",
    "def add_emoji_to_query(emoji_shortcut):\n",
    "    query_text_box.insert(tk.END, emoji_shortcut)\n",
    "    query_text_box.see(tk.END)\n",
    "\n",
    "# Function to parse the emoji query\n",
    "def parse_query():\n",
    "    emoji_query = query_text_box.get(\"1.0\", tk.END)\n",
    "    emoji_query_emojized = emoji.emojize(emoji_query)\n",
    "    sql_query = parse_emoji_query(emoji_query_emojized)\n",
    "    sql_query_box.delete(\"1.0\", tk.END)\n",
    "    sql_query_box.insert(tk.END, sql_query)\n",
    "    parse_sql_query(sql_query)\n",
    "\n",
    "# Set up the main window\n",
    "root = tk.Tk()\n",
    "root.title(\"EmojiQL GUI\")\n",
    "\n",
    "# Define the number of buttons per row\n",
    "buttons_per_row = 3\n",
    "\n",
    "# Create a text box for the emoji query with vertical scrollbar\n",
    "query_text_box = scrolledtext.ScrolledText(root, height=5, width=100)\n",
    "query_text_box.grid(row=0, column=0, columnspan=buttons_per_row, sticky='ew')\n",
    "scroll_bar = tk.Scrollbar(root, command=query_text_box.yview)\n",
    "scroll_bar.grid(row=0, column=buttons_per_row, sticky='ns')\n",
    "query_text_box['yscrollcommand'] = scroll_bar.set\n",
    "\n",
    "# Create a text box for the SQL query with vertical scrollbar\n",
    "sql_query_box = scrolledtext.ScrolledText(root, height=5, width=100)\n",
    "sql_query_box.grid(row=1, column=0, columnspan=buttons_per_row, sticky='ew')\n",
    "sql_scroll_bar = tk.Scrollbar(root, command=sql_query_box.yview)\n",
    "sql_scroll_bar.grid(row=1, column=buttons_per_row, sticky='ns')\n",
    "sql_query_box['yscrollcommand'] = sql_scroll_bar.set\n",
    "\n",
    "# Create buttons for each emoji and place them in a grid\n",
    "for idx, (shortcode, emoji_char) in enumerate(emoji_sql_mapping.items()):\n",
    "    button = tk.Button(root, text=f\"{shortcode} {emoji_char}\", command=lambda sc=shortcode: add_emoji_to_query(sc))\n",
    "    row, col = divmod(idx, buttons_per_row)\n",
    "    button.grid(row=2 + row, column=col, sticky='nsew', padx=5, pady=5)\n",
    "\n",
    "# Configure the grid to resize with the window\n",
    "for i in range(buttons_per_row):\n",
    "    root.grid_columnconfigure(i, weight=1)\n",
    "root.grid_rowconfigure(0, weight=0)\n",
    "root.grid_rowconfigure(1, weight=0)\n",
    "root.grid_rowconfigure(2, weight=1)\n",
    "\n",
    "# Create a button to parse the query\n",
    "parse_button = tk.Button(root, text=\"Parse Query\", command=parse_query, bg=\"green\", fg=\"white\")\n",
    "parse_button.grid(row=3 + (len(emoji_sql_mapping) // buttons_per_row), column=0, columnspan=buttons_per_row, sticky='ew')\n",
    "\n",
    "# Create button for reset\n",
    "reset_button = tk.Button(root, text=\"Reset\", command=reset, bg=\"red\", fg=\"white\")\n",
    "reset_button.grid(row=4 + (len(emoji_sql_mapping) // buttons_per_row), column=0, columnspan=buttons_per_row, sticky='ew')\n",
    "\n",
    "# Start the GUI event loop\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tkinter as tk\n",
    "# from tkinter import scrolledtext\n",
    "# import emoji\n",
    "\n",
    "# emoji_sql_mapping = {\n",
    "#     emoji.emojize(':magnifying_glass_tilted_right:'): 'SELECT',\n",
    "#     emoji.emojize(':file_folder:'): 'FROM',\n",
    "#     emoji.emojize(':red_question_mark:'): 'WHERE',\n",
    "#     emoji.emojize(':bar_chart:'): 'ORDER BY',\n",
    "#     emoji.emojize(':cross_mark:'): 'DELETE',\n",
    "#     emoji.emojize(':envelope_with_arrow:'): 'INSERT INTO',\n",
    "#     emoji.emojize(':pencil:'): 'VALUES',\n",
    "#     emoji.emojize(':newspaper:'): 'UPDATE',\n",
    "#     emoji.emojize(':check_box_with_check:'): 'SET',\n",
    "#     emoji.emojize(':plus:'): 'SUM',\n",
    "#     emoji.emojize(':money_with_wings:'): 'AVG',\n",
    "#     emoji.emojize(':input_numbers:'): 'COUNT',\n",
    "#     emoji.emojize(':down_arrow:'): 'MIN',\n",
    "#     emoji.emojize(':up_arrow:'): 'MAX',\n",
    "#     emoji.emojize(':handshake:'): 'JOIN',\n",
    "#     emoji.emojize(':speaker_low_volume::speaker_high_volume:'): 'ASC',\n",
    "#     emoji.emojize(':speaker_high_volume::speaker_low_volume:'): 'DESC',\n",
    "#     # Add more mappings as required\n",
    "# }\n",
    "\n",
    "# def parse_emoji_query(emoji_query):\n",
    "#     tokens = emoji.get_emoji_regexp().split(emoji_query)\n",
    "#     translated_tokens = [emoji_sql_mapping.get(token, token) for token in tokens]\n",
    "#     return ''.join(translated_tokens)\n",
    "\n",
    "# def add_emoji_to_query(emoji_shortcut):\n",
    "#     query_text_box.insert(tk.END, emoji_shortcut)\n",
    "#     query_text_box.see(tk.END)\n",
    "\n",
    "# def parse_query():\n",
    "#     emoji_query = query_text_box.get(\"1.0\", tk.END)\n",
    "#     emoji_query_emojized = emoji.emojize(emoji_query)\n",
    "#     sql_query = parse_emoji_query(emoji_query_emojized)\n",
    "#     sql_query_box.delete(\"1.0\", tk.END)\n",
    "#     sql_query_box.insert(tk.END, sql_query)\n",
    "\n",
    "# root = tk.Tk()\n",
    "# root.title(\"Emoji to SQL Parser\")\n",
    "\n",
    "# query_text_box = scrolledtext.ScrolledText(root, height=5, width=100)\n",
    "# query_text_box.pack(pady=10)\n",
    "\n",
    "# sql_query_box = scrolledtext.ScrolledText(root, height=5, width=100)\n",
    "# sql_query_box.pack(pady=10)\n",
    "\n",
    "# for shortcode, emoji_char in emoji_sql_mapping.items():\n",
    "#     button = tk.Button(root, text=f\"{shortcode} {emoji_char}\", command=lambda sc=shortcode: add_emoji_to_query(sc))\n",
    "#     button.pack(side=tk.LEFT)\n",
    "\n",
    "# parse_button = tk.Button(root, text=\"Parse Query\", command=parse_query, bg=\"green\", fg=\"white\")\n",
    "# parse_button.pack(pady=10)\n",
    "\n",
    "# root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 915 entries, 0 to 914\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   id            915 non-null    object\n",
      " 1   name          915 non-null    object\n",
      " 2   game_slug     915 non-null    object\n",
      " 3   price         915 non-null    int64 \n",
      " 4   release_date  915 non-null    object\n",
      " 5   platform      783 non-null    object\n",
      " 6   description   915 non-null    object\n",
      " 7   developer     712 non-null    object\n",
      " 8   publisher     707 non-null    object\n",
      " 9   genres        757 non-null    object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 811.2 KB\n",
      "None\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1765 entries, 4c81547b81064acfb1902be7b06d63661 to c7372a04d62b4d4bb5b2a95424202e252\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   id                  1615 non-null   object\n",
      " 1   operacional_system  1634 non-null   object\n",
      " 2   processor           1618 non-null   object\n",
      " 3   memory              1397 non-null   object\n",
      " 4   graphics            1341 non-null   object\n",
      " 5   fk_game_id          1765 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 961.2 KB\n",
      "None\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17584 entries, 0 to 17583\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   id          17584 non-null  object \n",
      " 1   company     17584 non-null  object \n",
      " 2   author      15769 non-null  object \n",
      " 3   rating      17053 non-null  float64\n",
      " 4   comment     17428 non-null  object \n",
      " 5   date        17584 non-null  object \n",
      " 6   top_critic  17584 non-null  bool   \n",
      " 7   game_id     17584 non-null  object \n",
      "dtypes: bool(1), float64(1), object(6)\n",
      "memory usage: 14.2 MB\n",
      "None\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3045 entries, 0 to 3044\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   id           3045 non-null   int64 \n",
      " 1   description  3045 non-null   object\n",
      " 2   url          3045 non-null   object\n",
      " 3   fk_game_id   3045 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 768.7 KB\n",
      "None\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   id            500 non-null    object\n",
      " 1   name          500 non-null    object\n",
      " 2   game_slug     500 non-null    object\n",
      " 3   price         500 non-null    int64 \n",
      " 4   release_date  500 non-null    object\n",
      " 5   platform      500 non-null    object\n",
      " 6   description   500 non-null    object\n",
      " 7   developer     500 non-null    object\n",
      " 8   publisher     500 non-null    object\n",
      " 9   genres        500 non-null    object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 458.8 KB\n",
      "None\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   id                  500 non-null    object\n",
      " 1   operacional_system  500 non-null    object\n",
      " 2   processor           500 non-null    object\n",
      " 3   memory              500 non-null    object\n",
      " 4   graphics            500 non-null    object\n",
      " 5   fk_game_id          500 non-null    object\n",
      "dtypes: object(6)\n",
      "memory usage: 251.8 KB\n",
      "None\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   id          500 non-null    object \n",
      " 1   company     500 non-null    object \n",
      " 2   author      500 non-null    object \n",
      " 3   rating      500 non-null    float64\n",
      " 4   comment     500 non-null    object \n",
      " 5   date        500 non-null    object \n",
      " 6   top_critic  500 non-null    bool   \n",
      " 7   game_id     500 non-null    object \n",
      "dtypes: bool(1), float64(1), object(6)\n",
      "memory usage: 408.2 KB\n",
      "None\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   id           500 non-null    int64 \n",
      " 1   description  500 non-null    object\n",
      " 2   url          500 non-null    object\n",
      " 3   fk_game_id   500 non-null    object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 126.7 KB\n",
      "None\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    }
   ],
   "source": [
    "memory_usage() # to check memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
